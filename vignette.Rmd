---
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: 3
    toc_float: yes
    number_sections: yes
  pdf_document:
    toc: yes
    toc_depth: '2'
  word_document:
    toc: yes
    toc_depth: '3'
bibliography: optic_refs.json
csl: https://www.zotero.org/styles/lancet
suppress-bibliography: no
link-citations: yes
urlcolor: blue
linkcolor: blue
editor_options:
  markdown:
    wrap: 72
---



```{r setup, include=FALSE}

library(data.table)
library(optic)
library(ggplot2)
library(formatR)

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)

```

# OPTIC Introduction {.unnumbered}

This vignette covers use of the OPTIC package, which contains tools to
simulate the performance of commonly-used statistical models. Briefly,
OPTIC uses Monte Carlo simulations to estimate the performance of models
typically used for state-level policy evaluation (for example,
differences-in-differences estimators). Package users provide the
simulation procedure with a policy evaluation scenario, a hypothetical
treatment effect, an estimator and model specification, and simulation
parameters. OPTIC then constructs simulations for all chosen simulation
parameters and estimators, returning summary statistics on each
simulation's performance.

Currently, OPTIC has been developed for policy evaluation scenarios with
the following assumptions:

1.  No confounding : There are no confounding variables which determine
    a observation's selection into treatment. Additionally, treatment
    effects are assumed to be constant rather than time-varying.
2.  Confounding due to co-occurring policies: There is one or more
    policies which co-occur with the the policy of-interest. Selection
    into treatment is still random.

Additional details on the simulation procedures within OPTIC are
available in Griffin et al., 2021
[@http://zotero.org/users/3390799/items/ZNCVTPJF] and Griffin et al.,
2022 [@http://zotero.org/users/3390799/items/V3Q6ARUA]. Forthcoming
updates to OPTIC will provides simulations which test the effect of
selection bias on model performance.

This README covers the use of OPTIC in R and provides examples of the
package's features. Section one covers package installation, section two
provides a general overview of the package's usage, and section three
provides two examples of the package under different policy scenarios.

# Installation

Clone the repository to your machine. Once you have the project on your
local machine, the first thing you will want to do is build and install
the package. Open the project in RStudio - it's easiest to open the
optic.Rproj file to do so. Make sure you have `devtools` installed.

# Overview of the OPTIC Package

OPTIC contains two core functions for performing simulations, which
require four user-provided inputs. Figure 1 below displays a flowchart
of OPTIC's required steps to produce simulation results.

![](optic_flowchart.pdf)

The following sections describe OPTIC inputs and steps in detail.

## User inputs

### Data

Analysts will need to shape their data to work properly with OPTIC. The
package contains an example dataset which demonstrates the format
required by the function "configure_simulation".

```{r, echo = FALSE}
knitr::kable(overdoses, format = "markdown")
```

The data should generally be structured to work as an input to a two-way
fixed effect model, using a "long-format", with variables for "group",
"time", covariates, and an outcome variable. The example data included
in the package is derived from data provided by the US Bureau of Labor
Statistics and the Centers for Disease Control and Prevention.

### Policy evaluation scenarios

Policy evaluation scenarios are single string inputs into the
"configure_simulation" function, representing either the no confounding
scenario ("noconf") or the confounding due to co-occurring policies
scenario ("concurrent"). For the "concurrent" scenario, there are
additional parameters required within "configure_simulation", which are
discussed in the "parameter" section below. Additional details on policy
evaluation scenarios are provided in Griffin et al.,
2021[\@<http://zotero.org/users/3390799/items/ZNCVTPJF>] and Griffin et
al., 2022[\@<http://zotero.org/users/3390799/items/V3Q6ARUA>].

### Treatment scenarios

This input represents the "true treatment effects" that OPTIC will
simulate across model iterations. OPTIC is currently designed to work on
static treatment effects (rather than dynamic treatment effects, such as
time-varying treatment effects). Users should structure their treatment
scenarios in a list, corresponding to a change in the outcome variable.
The list will contain two effects within a vector, if the user is
simulating models for the "concurrent" policy evaluation scenario. Using
the example_data:

```{r}

data(overdoses)

# Calculate 5% and 10% changes in mean opioid_death_rate, across states and years. 
five_percent_effect <- 0.05*mean(overdoses$crude.rate, na.rm = T)
ten_percent_effect  <- 0.10*mean(overdoses$crude.rate, na.rm = T)

# Calculate a confounding policy effect
confound_effect <- -0.02*mean(overdoses$crude.rate, na.rm = T)

# Scenario object for "no confounding" evaluation scenario:
scenarios_no_confounding <- list(five_percent_effect, ten_percent_effect)

# Scenario object for "co-occuring policy" evaluation scenario:
scenarios_co_occur <- list(c(five_percent_effect, confound_effect),
                           c(ten_percent_effect, confound_effect))
```

### List of models

For each treatment scenario, OPTIC will simulate a treatment effect and
then attempt to estimate this effect based on user-provided models. The
`configure_simulation` function takes a list of lists, with inner lists
requiring several named arguments. Model lists should contain:

-   `name`: A name for the model to identify the model type in results.

-   `type`: Users can set this as either "autoreg", "drdid",
    "multisynth", or "reg".

    -   "reg" uses a typical regression framework, implementing the
        procedure chosen in `model_call`.

    -   "autoreg" adds a dependent lag to the model formula.

    -   "drdid" estimates a treatment effect using a doubly-robust
        difference-in-difference estimator, with covariates in the
        `model_formula` argument used within both the propensity score
        stage and (for more details on doubly robust
        difference-in-differences, see Sant'Anna \\& Zhao, 2020)

    -   "multisynth" estimates a treatment effect using augmented
        synthetic controls (for additional details on augmented
        synthetic controls, see Ben-Michael, Feller, \\& Rothstein,
        2021).

-   `call`: The call for the model in R (e.g. "lm", "glm", etc).

-   `formula`: The model specification, in an R formula. Needs to
    include a variable labeled "treament_level" for treatment status
    or labeled "treatment_change" for coding a change in treatment status (e.g. 
    used in autoregressive models). For "concurrent" scenarios,
    treatment variables should labeled "treatment1", "treatment2", . . .,
    "treatment{n}" for each policy included.

-   `args`: Any additional arguments passed to the model_call
    (e.g. "weights", "family", "control" etc.).

-   `se_adjust`: Any adjustments to standard errors after estimation.
    OPTIC recognizes either "none" for no adjustment or "cluster" for
    clustered standard errors (OPTIC will use the `configure_simulation`
    parameter "unit_var" to determine clusters for clustered standard
    errors). \# Add additional se

Below provides an example of a model list using the `example_data`:

```{r}

lm_fe <- optic_model(
    name = "fixed_effect_linear",
    type = "reg",
    call = "lm",
    formula = crude.rate ~ unemploymentrate + as.factor(year) + as.factor(state) + treatment1_level + treatment2_level,
    args=list(weights = as.name('population')),
    se_adjust = "cluster"
)
  
lm_ar <- optic_model(
    name = "Auto-regressive linear",
    type = "autoreg",
    call = "lm",
    formula = crude.rate ~ unemploymentrate + year + treatment1_change + treatment2_change,
    args=list(),
    se_adjust = "cluster"
)

sim_models <- list(lm_fe, lm_ar)

```

## OPTIC functions

### `configure_simulation`

This function takes the policy evaluation scenario, treatment scenarios,
model list, and function parameters to generate synthetic datasets with
simulated treatment effects. `configure_simulation` has the following
 arguments:

-   `x`: The prepped analysis dataset. See section above for more
    details.

-   `models`: List of models. See section above for more details.

-   `method`: Policy evaluation scenario. Can either be "noconf" or
    "concurrent".

-   `unit_var`: Variable for groups within the dataset. Used to
    determine clusters for clustered standard errors

-   `time_var`: The variable used for time units.

-   `effect_magnitude`: A vector of treatment scenarios. See section
    above for more details. Synthetic datasets will be generated for
    each entry in the vector.

-   `n_units`: A vector with the number of units to simulate treatment.
    Synthetic datasets will be generated for each entry in the vector.

-   `effect_direction`: A vector containing either 'neg', 'null', or
    'pos'. Synthetic datasets will be generated for each entry in the
    vector. Determines the direction of the simulated effect.

-   `policy_speed`: A vector containing either 'instant' or 'slow'
    entries, determining how quickly treated units obtain the simulated
    effect. Synthetic datasets will be generated for each entry in the
    vector. Can either be 'instant" (so treatment effect applies fully
    in the first treated time period) or 'slow' (treatment effect ramps
    up linearly to the desired effect size, based on
    `n_implementation_periods`.

-   `n_implementation_periods` A vector with number of periods after
    implementation until treated units reach the desired simulated
    treatment effect. Synthetic datasets will be generated for each
    entry in the vector.

Three arguments to `configure_simulation` only apply within the
"concurrent" policy scenario:

-   `rhos`: A vector of 0-1 values indicating the correlation between
    the primary policy and a confounding policy. Synthetic datasets will
    be generated for each entry in the vector.

-   `years_apart`: Number of years between the primary policy being
    implemented and the confounding policy.

-   `ordered`: Determines if the primary policy always occurs before the
    confounding policy (`TRUE`) or if the policies are randomly ordered
    (`FALSE`).

The function returns a configuration object that's used as an input to
`dispatch_simulation`. This object contains a dataset listing all
possible simulations that will be run for each model (. An example call
of `configure_simulation` is displayed below:

```{r}

sim_config <- optic_simulation(
  
  x      = overdoses,
  models = sim_models,
  iters  = 10,
  method = "concurrent",

  unit_var                 = "state",
  time_var                 = "year",
  effect_magnitude         = scenarios_co_occur,
  n_units                  = c(30),
  effect_direction         = c("neg"),
  policy_speed             = c("instant", "slow"),
  n_implementation_periods = c(3),
  rhos                     = c(0.5),
  years_apart              = 2,
  ordered                  = TRUE

)

```

### `simulate`

With the configuration object built, we can now simulate models with the configuration scenarios using the `simulate` function. This function accepts arguments to parallelize code through the use of the "future" package. If this setting is turned on, the function will try to parallelize simulation iterations on the analyst's machine. Any additional packages required by the user for model calls (e.g. a specific modeling package) must also be passed to dispatch_simulations using future.globals and future.packages arguments.


```{r}

results <- dispatch_simulations(
  sim_config,
  use_future = FALSE,
  seed = 9782,
  verbose = 2
)

```
OPTIC will provide a short summary on the models being run if verbose is set to 1 or 2. We now have results which can be used to construct summary statistics and compare models for policy evaluation. Each scenario will be saved as a list entry in the results object, with each list entry containing a dataframe with rows for model results. Model results returns model point estimates & standard errors for each treatment variable (in the 'noconf' method, this variable would be 'treatment' and for the 'concurrent" method, this variable would be treatment1 & treatment2), along with policy scenario settings.  

```{r}

results

```

We can use the results table to analyze the relative performance of models across data scenarios or create test statistics as needed for an analysis. For example, we might be interested in comparing  relative bias across the policy-1 point estimate, for the linear fixed-effect model against the autoregressive linear model, in the scenario where the effect of policy implementation is immediate and decreases the crude.rate by 5%:

```{r}

# Compare point estimates across models for the 5% change scenario, with instantaneous policy adoption:
df_compare <- results[[1]][results[[1]]$se_adjustment == "cluster",]

true_est <- -five_percent_effect

mod1_pred <- mean(df_compare[df_compare$model_name == "fixed_effect_linear",]$estimate1)
mod2_pred <- mean(df_compare[df_compare$model_name == "Auto-regressive linear",]$estimate1)

print(paste0("True effect size: ", round(true_est, 2)))
print(paste0("FE LM effect: ", round(mod1_pred, 2)))
print(paste0("AR LM effect: ", round(mod2_pred, 2)))

```

From the above output, we can see the fixed effect model is underestimating the true effect size for this scenario while the auto-regressive model is overestimating the effect size. 

#### Bibliography
